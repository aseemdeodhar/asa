{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Spatial Analysis\n",
    "# Module 08: APIs, Geocoding, Geolocation\n",
    "\n",
    "You'll need a Google API key to use the Google Maps Geocoding API and the Google Places API Web Service. These APIs require you to set up billing info, but we won't use them beyond the free threshold. Complete the following steps before the class session.\n",
    "\n",
    "  1. Go to the Google API console: https://console.developers.google.com/\n",
    "  1. Sign in, create a new project for class, then click enable APIs.\n",
    "  1. Enable the Google Maps Geocoding API and then the Google Places API.\n",
    "  1. Go to credentials, create an API key, then copy it.\n",
    "  1. Create a new file (in the same folder as this notebook) called `keys.py` with one line: `google_api_key = 'PASTE-YOUR-KEY-HERE'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from geopy.geocoders import GoogleV3\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from keys import google_api_key\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a pause duration between API requests\n",
    "pause = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Geocoding addresses to lat-long\n",
    "\n",
    "We will use the Google Maps geocoding API. Documentation: https://developers.google.com/maps/documentation/geocoding/start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.DataFrame()\n",
    "locations['address'] = ['350 5th Ave, New York, NY 10118',\n",
    "                        '100 Larkin St, San Francisco, CA 94102',\n",
    "                        'Snell Library, Boston, MA']\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that accepts an address string, sends it to the Google API, and returns the lat-long API result\n",
    "def geocode(address):\n",
    "    time.sleep(pause) #pause for some duration before each request, to not hammer their server\n",
    "    url_template = 'https://maps.googleapis.com/maps/api/geocode/json?address={}&key={}&sensor=false' #api url with placeholders\n",
    "    url = url_template.format(address, google_api_key) #fill in the placeholder with a variable\n",
    "    response = requests.get(url) #send the request to the server and get the response\n",
    "    data = response.json() #convert the response json string into a dict\n",
    "    \n",
    "    if len(data['results']) > 0: #if google was able to geolocate our address, extract lat-long from result\n",
    "        latitude = data['results'][0]['geometry']['location']['lat']\n",
    "        longitude = data['results'][0]['geometry']['location']['lng']\n",
    "        return '{},{}'.format(latitude, longitude) #return lat-long as a string in the format google likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function (you can provide famous site names instead of addresses)\n",
    "geocode('Fenway Park, Boston, MA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each value in the address column, geocode it, save results as new df column\n",
    "locations['latlng'] = locations['address'].map(geocode)\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the result into separate lat and lon columns for easy mapping\n",
    "locations['latitude'] = locations['latlng'].map(lambda x: x.split(',')[0])\n",
    "locations['longitude'] = locations['latlng'].map(lambda x: x.split(',')[1])\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# create a new pandas series of 3 addresses and use our function to geocode them\n",
    "# create new variables to contain your work so as to not overwrite the locations df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# create a new pandas series of 3 famous site names and use our function to geocode them\n",
    "# create new variables to contain your work so as to not overwrite the locations df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Google Places API\n",
    "\n",
    "We will use Google's Places API to look up places in the vicinity of some location. Documentation: https://developers.google.com/places/web-service/intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google places API URL, with placeholders\n",
    "url_template = 'https://maps.googleapis.com/maps/api/place/search/json?keyword={}&location={}&radius={}&key={}&sensor=false'\n",
    "\n",
    "# what keyword to search for\n",
    "keyword = 'restaurant'\n",
    "\n",
    "# define the radius (in meters) for the search\n",
    "radius = 500\n",
    "\n",
    "# get the location coordinates (of snell library)\n",
    "location = locations.loc[2, 'latlng']\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add our variables into the url, submit the request to the api, and load the response\n",
    "url = url_template.format(keyword, location, radius, google_api_key)\n",
    "response = requests.get(url)\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many results did we get?\n",
    "len(data['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the first 3\n",
    "data['results'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the results into a dataframe of places\n",
    "places = pd.DataFrame(data['results'], columns=['name', 'geometry', 'rating', 'vicinity'])\n",
    "places.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse out lat-long and return it as a series -> this creates a dataframe of all the results when you .apply()\n",
    "def parse_coords(geometry):\n",
    "    if isinstance(geometry, dict):\n",
    "        lng = geometry['location']['lng']\n",
    "        lat = geometry['location']['lat']\n",
    "        return pd.Series({'latitude':lat, 'longitude':lng})\n",
    "    \n",
    "# test our function\n",
    "places['geometry'].head().apply(parse_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now run our function on the whole dataframe and save the output to 2 new dataframe columns\n",
    "places[['latitude', 'longitude']] = places['geometry'].apply(parse_coords)\n",
    "places_clean = places.drop('geometry', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the places by rating\n",
    "places_clean = places_clean.sort_values(by='rating', ascending=False)\n",
    "places_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# find the five highest-rated bars within 1/2 mile of fenway park\n",
    "# create new variables to contain your work so as to not overwrite places and places_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reverse geocoding (address lookup)\n",
    "\n",
    "We'll use Google's reverse geocoding API. Documentation: https://developers.google.com/maps/documentation/geocoding/intro#ReverseGeocoding\n",
    "\n",
    "You can do this manually, just like in the previous two sections, but it's a little more complicated to parse Google's address components results. If we just want addresses, we can use [geopy](https://geopy.readthedocs.io/) to simply call Google's API automatically for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity, we'll use the points from the Places API, but you could load any points dataset here\n",
    "points = places_clean.loc[:, ['latitude', 'longitude']]\n",
    "points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column to put lat-long into the format google likes - this just makes it easier to call their API\n",
    "points['latlng'] = points.apply(lambda row: '{},{}'.format(row['latitude'], row['longitude']), axis=1)\n",
    "points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell geopy to reverse geocode some lat-long string using Google's API and return the address\n",
    "def reverse_geopy(latlng):\n",
    "    time.sleep(pause)\n",
    "    geolocator = GoogleV3(api_key=google_api_key)\n",
    "    address, _ = geolocator.reverse(latlng, exactly_one=True)\n",
    "    return address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now reverse-geocode the points to addresses\n",
    "points['address'] = points['latlng'].map(reverse_geopy)\n",
    "points.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if you just want the city or state?\n",
    "You could try to parse the address strings, but you're relying on them always having a consistent format. This might not be the case if you have international location data. In this case, you should call the API manually and extract the individual address components you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the Google API latlng data to reverse geocode it\n",
    "def reverse_geocode(latlng):\n",
    "    time.sleep(pause)\n",
    "    url_template = 'https://maps.googleapis.com/maps/api/geocode/json?latlng={}&key={}'\n",
    "    url = url_template.format(latlng, google_api_key)\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    if len(data['results']) > 0:\n",
    "        return data['results'][0] #if we got results, return the first result\n",
    "    \n",
    "geocode_results = points['latlng'].map(reverse_geocode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocode_results.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look inside each reverse geocode result to see if address_components exists. If it does, look inside each component to see if we can find the city or the state. Google calls the city name by the abstract term 'locality' and the state name by the abstract term 'administrative_area_level_1' ...this just lets them use the same terminology anywhere in the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city(geocode_result):\n",
    "     if 'address_components' in geocode_result:\n",
    "        for address_component in geocode_result['address_components']:\n",
    "            if 'locality' in address_component['types']:\n",
    "                return address_component['long_name']\n",
    "                \n",
    "def get_state(geocode_result):\n",
    "     if 'address_components' in geocode_result:\n",
    "        for address_component in geocode_result['address_components']:\n",
    "            if 'administrative_area_level_1' in address_component['types']:\n",
    "                return address_component['long_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now map our functions to extract city and state names\n",
    "points['city'] = geocode_results.map(get_city)                \n",
    "points['state'] = geocode_results.map(get_state)\n",
    "points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# write a new function get_neighborhood() to parse the neighborhood name and add it to the points df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reverse geocoding to FIPS\n",
    "\n",
    "We'll use the FCC's Census Block Conversions API to turn lat/long into a block FIPS code. FIPS codes contain from left to right: the location's 2-digit state code, 3-digit county code, 6-digit census tract code, and 4-digit census block code (the first digit of which is the census block group code). Now you can join your data to tract (etc) level census data without doing a spatial join.\n",
    "\n",
    "  - Documentation: https://geo.fcc.gov/api/census/\n",
    "  - Example request: https://geo.fcc.gov/api/census/block/find?format=json&latitude=42.340970&longitude=-71.081658\n",
    "  \n",
    "You can do similar work with the census geocoder: https://geocoding.geo.census.gov/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the FCC API lat/long and get FIPS data back - return block fips and county name\n",
    "def get_fips(row):\n",
    "    time.sleep(pause)\n",
    "    url_template = 'https://geo.fcc.gov/api/census/block/find?format=json&latitude={}&longitude={}'\n",
    "    url = url_template.format(row['latitude'], row['longitude'])\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # return values as a series: when applied, this will create a dataframe with multiple columns\n",
    "    return pd.Series({'fips_code':data['Block']['FIPS'], 'county':data['County']['name']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get block fips code and county name from FCC as new dataframe\n",
    "fips = points.apply(get_fips, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate to join points df and new fips/county df\n",
    "points_fips = pd.concat([points, fips], axis=1)\n",
    "points_fips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# take your geocoded series from section 1 and reverse-geocode it to get block fips codes\n",
    "# then parse out the tract fips code from each row and save as a new series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Other APIs and Data Portals\n",
    "\n",
    "Using the Cambridge Open Data Portal... browse the portal for public datasets: https://data.cambridgema.gov/browse\n",
    "\n",
    "The API is built on Socrata... documentation: https://dev.socrata.com/\n",
    "\n",
    "First we'll look at tax assessor data in Cambridge: https://data.cambridgema.gov/Assessing/Assessing-Building-Information-FY2015/crnm-mw9n\n",
    "\n",
    "### 5.1. Tax assessor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define API endpoint\n",
    "endpoint_url = 'https://data.cambridgema.gov/resource/crnm-mw9n.json'\n",
    "\n",
    "# request the URL and download its response\n",
    "response = requests.get(endpoint_url)\n",
    "\n",
    "# parse the json string into a Python dict\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more than 1000 rows in the dataset, but we're limited by the API to only 1000 per request. We have to use pagination to get the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursive function to keep requesting more rows until there are no more\n",
    "def request_data(endpoint_url, limit=1000, offset=0, data=[]):\n",
    "    \n",
    "    url = endpoint_url + '?$limit={limit}&$offset={offset}'\n",
    "    request_url = url.format(limit=limit, offset=offset)\n",
    "    response = requests.get(request_url)\n",
    "    \n",
    "    rows = response.json()\n",
    "    data.extend(rows)\n",
    "    \n",
    "    if len(rows) >= limit:\n",
    "        data = request_data(endpoint_url, offset=offset+limit, data=data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the data from the API, using our recursive function\n",
    "endpoint_url = 'https://data.cambridgema.gov/resource/crnm-mw9n.json'\n",
    "data = request_data(endpoint_url)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turn the json data into a dataframe\n",
    "df = pd.DataFrame(data)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what columns are in our data?\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the assessed values\n",
    "df['assessed_value'].dropna().astype(int).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the years built\n",
    "built = df['actual_year_built'].dropna().astype(int)\n",
    "built.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = built[built > 1600].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# what is the mean year built for the 10 properties with the highest assessed value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now map the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloaded from https://data.cambridgema.gov/api/geospatial/rst6-227j?method=export&format=GeoJSON\n",
    "parcels = gpd.read_file('data/parcels.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge parcel geometries with assessor dataset\n",
    "parcels_assess = pd.merge(parcels, df, how='left', left_on='ml', right_on='gis_id')\n",
    "parcels_assess[['assessed_value', 'land_area', 'living_area']] = parcels_assess[['assessed_value', 'land_area', 'living_area']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate value per sq ft (living area + land area), then drop inifinities and nulls\n",
    "parcels_assess['value_per_area'] = parcels_assess['assessed_value'] / (parcels_assess['living_area'] + parcels_assess['land_area'])\n",
    "parcels_assess = parcels_assess.replace([np.inf, -np.inf], np.nan).dropna(subset=['value_per_area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip outliers to min/max values at the 1 percentile and 99 percentile\n",
    "lower = parcels_assess['value_per_area'].quantile(0.01)\n",
    "upper = parcels_assess['value_per_area'].quantile(0.99)\n",
    "parcels_assess['value_per_area'] = parcels_assess['value_per_area'].clip(lower=lower, upper=upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the parcels\n",
    "fig, ax = plt.subplots(figsize=(10,10), facecolor='k')\n",
    "ax = parcels_assess.plot(ax=ax, column='value_per_area')\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# choose another variable from the geodataframe and map it: do you notice any clusters or trends you can explain?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Crash data\n",
    "\n",
    "https://data.cambridgema.gov/Public-Safety/Police-Department-Crash-Data-Historical/ybny-g9cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the data from the API, using our recursive function\n",
    "endpoint_url = 'https://data.cambridgema.gov/resource/39tu-m8zx.json'\n",
    "data = request_data(endpoint_url)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the json data into a dataframe\n",
    "df = pd.DataFrame(data)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn all the rows with lat-lng data into a geopandas geoseries of points\n",
    "df_geo = df[pd.notnull(df['latitude']) & pd.notnull(df['longitude'])]\n",
    "df_geo = df_geo[['latitude', 'longitude']].astype(float)\n",
    "crash_points = gpd.GeoSeries(df_geo.apply(lambda row: Point((row['longitude'], row['latitude'])), axis=1))\n",
    "len(crash_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now map it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapefiles downloaded from https://www.census.gov/cgi-bin/geo/shapefiles/index.php\n",
    "cities = gpd.read_file('data/tl_2018_25_place/')\n",
    "tracts = gpd.read_file('data/tl_2018_25_tract/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cambridge's boundaries\n",
    "cambridge_polygon = cities[cities['NAME'].str.contains('Cambridge')]['geometry'].iloc[0]\n",
    "cambridge_polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do our CRSs match? we need them to, to do spatial analysis\n",
    "cities.crs == tracts.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how many tracts are entirely within cambridge's boundaries?\n",
    "cambridge_tracts = tracts[tracts.within(cambridge_polygon)]\n",
    "len(cambridge_tracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the tracts and the crash points\n",
    "ax = cambridge_tracts.plot(color='w', edgecolor='gray')\n",
    "ax = crash_points.plot(ax=ax, color='r', markersize=0.1)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the crash points into a geodataframe and project to the tracts' CRS\n",
    "gdf_crashes = gpd.GeoDataFrame(geometry=crash_points)\n",
    "gdf_crashes.crs = {'init':'epsg:4326'}\n",
    "gdf_crashes = gdf_crashes.to_crs(cambridge_tracts.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join tracts to crashes (i.e., assign a tract ID to each crash)\n",
    "crash_tracts = gpd.sjoin(gdf_crashes, cambridge_tracts, how='left', op='intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which tracts contain the most crashes?\n",
    "tract_crash_counts = crash_tracts['GEOID'].value_counts()\n",
    "tract_crash_counts.name = 'crashes'\n",
    "tract_crash_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the crash counts and the tracts\n",
    "cambridge_tracts = cambridge_tracts.set_index('GEOID')\n",
    "cambridge_tracts_crashes = pd.merge(cambridge_tracts, tract_crash_counts, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many crashes per square meter?\n",
    "cambridge_tracts_crashes['crash_density'] = cambridge_tracts_crashes['crashes'] / cambridge_tracts_crashes['ALAND']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the count of crashes per tract\n",
    "fig, ax = plt.subplots(facecolor='k')\n",
    "ax = cambridge_tracts_crashes.plot(ax = ax, column='crashes')\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the crashes/m2 per tract\n",
    "fig, ax = plt.subplots(facecolor='k')\n",
    "ax = cambridge_tracts_crashes.plot(ax = ax, column='crash_density')\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class exercise\n",
    "\n",
    "1. Visit the Cambridge data portal (link provided above) and identify another data set of interest (pick one with spatial data like lat/longs or polygon boundaries)\n",
    "1. Download it using Python as we did above\n",
    "1. Clean the data set if necessary and calculate descriptive stats for 2 or more columns\n",
    "1. Map the data, colored by column values. Do you see any patterns of interest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
